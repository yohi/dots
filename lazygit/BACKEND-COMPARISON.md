# AI Backend Comparison

Quick reference to help you choose the right AI backend for your needs.

## At a Glance

| Feature | Gemini | Claude | Ollama | Mock |
|---------|--------|--------|--------|------|
| **Speed** | âš¡âš¡âš¡ Fast (1-3s) | âš¡âš¡ Medium (2-5s) | âš¡ Slow (5-15s) | âš¡âš¡âš¡ Instant |
| **Quality** | â­â­â­ Good | â­â­â­â­ Excellent | â­â­ Fair | â­ Basic |
| **Cost** | ðŸ’° Free tier + cheap | ðŸ’°ðŸ’° Paid only | ðŸ†“ Free | ðŸ†“ Free |
| **Privacy** | â˜ï¸ Cloud | â˜ï¸ Cloud | ðŸ”’ Local | ðŸ”’ Local |
| **Setup** | âœ… Easy | âœ… Easy | âš™ï¸ Moderate | âœ… None |
| **Internet** | âœ… Required | âœ… Required | âŒ Not needed | âŒ Not needed |
| **Best For** | Most users | Quality-focused | Privacy-sensitive | Testing |

## Detailed Comparison

### Speed

**Gemini** (1-3 seconds)
- Fastest cloud option
- Optimized for quick responses
- Rarely times out
- âœ… Best for: Fast-paced development

**Claude** (2-5 seconds)
- Slightly slower than Gemini
- Still very responsive
- Consistent performance
- âœ… Best for: When quality > speed

**Ollama** (5-15 seconds)
- Depends on your hardware
- Faster with GPU acceleration
- Can be slow on older machines
- âœ… Best for: When you can wait for privacy

**Mock** (< 1 second)
- Instant responses
- No AI processing
- Simple heuristics
- âœ… Best for: Testing and CI/CD

### Quality

**Claude** â­â­â­â­
- Best understanding of code context
- Most accurate commit messages
- Excellent at following Conventional Commits
- Rarely produces off-topic messages
- âœ… Best for: Professional projects

**Gemini** â­â­â­
- Good quality overall
- Occasionally generic messages
- Usually follows format correctly
- Good balance of speed and quality
- âœ… Best for: Most projects

**Ollama** â­â­
- Quality varies by model
- Mistral: decent quality
- CodeLlama: better for code
- May need prompt tuning
- âœ… Best for: When privacy matters more than perfection

**Mock** â­
- Basic keyword matching
- Generic messages
- No real understanding
- Consistent but simple
- âœ… Best for: Testing only

### Cost

**Ollama** ðŸ†“
- Completely free
- Only electricity costs
- No usage limits
- No API keys needed
- âœ… Best for: Budget-conscious or high-volume users

**Gemini** ðŸ’°
- Generous free tier (15 req/min)
- Very cheap paid tier ($0.00025/1K chars)
- ~$0.01/month for typical usage
- ~$0.10/month for heavy usage
- âœ… Best for: Most users (essentially free)

**Claude** ðŸ’°ðŸ’°
- No free tier
- Haiku: $0.25/million tokens
- ~$0.25/month for typical usage
- ~$1.50/month for heavy usage
- âœ… Best for: When quality justifies cost

**Mock** ðŸ†“
- Free
- No costs at all
- âœ… Best for: Testing

### Privacy & Security

**Ollama** ðŸ”’ðŸ”’ðŸ”’
- Everything stays local
- No data sent externally
- Full control over data
- Complies with strict policies
- âœ… Best for: Proprietary/sensitive code

**Mock** ðŸ”’ðŸ”’
- Local processing
- No external calls
- No data collection
- âœ… Best for: Testing sensitive projects

**Gemini** â˜ï¸
- Code sent to Google servers
- Subject to Google's privacy policy
- Data may be used for improvements
- âš ï¸ Review diffs before generating
- âœ… Best for: Open source or non-sensitive code

**Claude** â˜ï¸
- Code sent to Anthropic servers
- Subject to Anthropic's privacy policy
- Data not used for training (per policy)
- âš ï¸ Review diffs before generating
- âœ… Best for: Open source or non-sensitive code

### Setup Difficulty

**Mock** âœ…âœ…âœ…
- No setup required
- Works out of the box
- No dependencies
- âœ… Best for: Quick testing

**Gemini** âœ…âœ…
- One command: `pip install google-generativeai`
- Get API key from website
- Set environment variable
- 5 minutes total
- âœ… Best for: Quick production setup

**Claude** âœ…âœ…
- One command: `npm install -g @anthropic-ai/claude-cli`
- Get API key from website
- Add credits to account
- Set environment variable
- 10 minutes total
- âœ… Best for: When you want best quality

**Ollama** âš™ï¸
- Install Ollama
- Pull model (large download)
- Start service
- Configure to run on boot
- 15-30 minutes total
- âœ… Best for: Long-term privacy solution

### Internet Requirement

**Cloud (Gemini/Claude)**
- âœ… Requires stable internet
- âŒ Won't work offline
- âš ï¸ Affected by network issues
- âš ï¸ Blocked by some corporate firewalls

**Local (Ollama/Mock)**
- âœ… Works offline
- âœ… No network dependency
- âœ… Works behind firewalls
- âœ… No latency from network

## Use Case Recommendations

### Individual Developer (Open Source)
**Recommended: Gemini**
- Fast and free
- Good quality
- Easy setup
- Code is public anyway

### Individual Developer (Private Projects)
**Recommended: Ollama**
- Privacy-focused
- No recurring costs
- Works offline

### Small Team
**Recommended: Gemini**
- Cost-effective
- Consistent quality
- Easy for everyone to set up

### Enterprise/Corporate
**Recommended: Ollama**
- Meets compliance requirements
- No data leaving network
- No per-user costs
- Full control

### Quality-Focused Developer
**Recommended: Claude**
- Best commit messages
- Worth the small cost
- Professional results

### High-Volume User (200+ commits/day)
**Recommended: Ollama**
- No API costs
- No rate limits
- Consistent performance

### Testing/CI/CD
**Recommended: Mock**
- No API keys in CI
- Fast and reliable
- No external dependencies

### Offline Developer
**Recommended: Ollama**
- Only option that works offline
- No internet required

## Migration Path

### Start â†’ Grow â†’ Scale

**Phase 1: Testing**
```bash
export AI_BACKEND=mock
```
- Test the integration
- Learn the workflow
- No setup required

**Phase 2: Production (Individual)**
```bash
export AI_BACKEND=gemini
export GEMINI_API_KEY="your-key"
```
- Real AI quality
- Free tier sufficient
- Easy to set up

**Phase 3: Scale (Team/Enterprise)**
```bash
export AI_BACKEND=ollama
ollama serve
```
- Privacy compliant
- Cost-effective at scale
- Full control

## Quick Decision Tree

```
Do you need it for testing only?
â”œâ”€ Yes â†’ Mock
â””â”€ No â†’ Do you have privacy/compliance requirements?
    â”œâ”€ Yes â†’ Ollama
    â””â”€ No â†’ Do you need the absolute best quality?
        â”œâ”€ Yes â†’ Claude
        â””â”€ No â†’ Gemini
```

## Switching Backends

You can easily switch between backends:

```bash
# Try Gemini
export AI_BACKEND=gemini
lazygit

# Switch to Ollama
export AI_BACKEND=ollama
lazygit

# Back to mock for testing
export AI_BACKEND=mock
lazygit
```

No code changes needed - just change the environment variable!

## Performance Benchmarks

Based on typical usage (500 byte diff):

| Backend | Avg Time | P95 Time | Timeout Rate |
|---------|----------|----------|--------------|
| Gemini | 1.5s | 3s | < 0.1% |
| Claude | 2.5s | 5s | < 0.1% |
| Ollama (CPU) | 8s | 15s | < 1% |
| Ollama (GPU) | 4s | 8s | < 0.5% |
| Mock | 0.1s | 0.2s | 0% |

## Cost Estimates

Based on 50 commits/day, 500 bytes avg diff:

| Backend | Daily Cost | Monthly Cost | Yearly Cost |
|---------|------------|--------------|-------------|
| Gemini (free) | $0 | $0 | $0 |
| Gemini (paid) | $0.0003 | $0.01 | $0.12 |
| Claude | $0.008 | $0.25 | $3.00 |
| Ollama | $0 | $0 | $0 |
| Mock | $0 | $0 | $0 |

## Summary

**Most users should start with Gemini** - it's fast, free, and good quality.

**Switch to Ollama if**:
- You work on sensitive/proprietary code
- You need to work offline
- You make 200+ commits per day
- Your organization requires it

**Upgrade to Claude if**:
- You need the absolute best quality
- You're willing to pay for perfection
- You're working on critical projects

**Use Mock for**:
- Testing the integration
- CI/CD pipelines
- Demonstrating the feature

## Need Help Deciding?

Ask yourself:

1. **Is my code sensitive?** â†’ Yes = Ollama, No = continue
2. **Do I need best quality?** â†’ Yes = Claude, No = continue
3. **Do I want free?** â†’ Yes = Gemini

Still unsure? **Start with Gemini** - you can always switch later!
